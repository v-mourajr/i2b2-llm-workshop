{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![i2b2 Logo](images/transmart-logo.png)\n",
    "\n",
    "# Using LLM + Embeddings to Search Patient Notes (RAG - Basics)\n",
    "\n",
    "This notebook demonstrates how to use **local embeddings** and a **Retrieval-Augmented Generation (RAG)** approach to search and analyze clinical notes stored in an i2b2-like format. You'll learn how to decode raw notes, embed them using the MiniLM model, perform semantic search with FAISS, and generate structured clinical responses using a local LLM (e.g., Qwen or LLaMA 3 via Ollama).\n",
    "\n",
    "### üîç Key Concepts Covered\n",
    "\n",
    "- Decoding BinHex-encoded clinical notes\n",
    "- Creating semantic vector embeddings with `MiniLM`\n",
    "- Storing embeddings in a FAISS vector store (in memory)\n",
    "- Performing similarity search and interpreting cosine similarity scores\n",
    "- Filtering to include only the **most relevant and recent** patient notes\n",
    "- Injecting retrieved context into a structured prompt template\n",
    "- Using a local LLM (Ollama) to generate clinically relevant summaries\n",
    "\n",
    "Each cell builds on the previous one to demonstrate a complete, hands-on RAG pipeline adapted for **clinical informatics** use cases using familiar i2b2-style data.\n"
   ],
   "id": "e6022b84adefe37b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare Data for Embedding\n",
    "\n",
    "Before we can search and analyze clinical notes using vector similarity, we need to prepare the data:\n",
    "\n",
    "- **1.1**: Load the i2b2-mimicking dataset containing BinHex-encoded clinical notes.\n",
    "- **1.2**: Decode the notes and add a new column (`note_text`) with plain-text content.\n",
    "\n",
    "This prepares the dataset for the next step, where we will embed the notes into a vector space using a local MiniLM model.\n"
   ],
   "id": "b61f5e06bc86b7ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1.1. Load and Explore Visit Data from i2b2-Mimicking CSV\n",
    "# -----------------------------------------------------------\n",
    "# This cell loads clinical visit data from a CSV that simulates the i2b2\n",
    "# `visit_dimension` table. Each row contains metadata and a clinical note\n",
    "# encoded in BinHex format.\n",
    "\n",
    "# Fields included:\n",
    "# - encounter_num: Unique visit ID\n",
    "# - patient_num: Patient identifier\n",
    "# - start_date, end_date: Visit dates\n",
    "# - location_cd, location_path: Care location details\n",
    "# - visit_blob: BinHex-encoded clinical note text\n",
    "\n",
    "import pandas as pd\n",
    "# from mistune import markdown\n",
    "\n",
    "# Define the path to the simulated i2b2 CSV file\n",
    "csv_path = \"datafiles/i2b2_encounter_table.csv\"\n",
    "\n",
    "# Load the data into a pandas DataFrame\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Display the first 10 rows to inspect the structure\n",
    "df.head(10)\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1.2. Decode BinHex Clinical Notes and Prepare Text Corpus\n",
    "# -----------------------------------------------------------\n",
    "# This cell decodes the clinical notes stored in BinHex format and\n",
    "# adds a new `note_text` column containing plain-text notes.\n",
    "# These will be used for embedding in the next step.\n",
    "\n",
    "import binascii\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Function to decode a single BinHex string\n",
    "def decode_note(hex_blob):\n",
    "    hex_str = hex_blob.replace(\"0x\", \"\")\n",
    "    return binascii.unhexlify(hex_str).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# Apply decoding to all rows\n",
    "df[\"note_text\"] = df[\"visit_blob\"].apply(decode_note)\n",
    "\n",
    "# Display the first 10 decoded records\n",
    "display(df.head(10))\n",
    "\n",
    "# Display an example decoded note\n",
    "example_index = 10\n",
    "display(Markdown(f\"### Decoded Note Example (Row {example_index} of {len(df)}):\\n\\n```text\\n{df['note_text'][example_index]}\\n```\"))\n"
   ],
   "id": "ab9f90d9cadb0592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Decode, Embed and Store Clinical Notes in a FAISS Vector Store (In-Memory)\n",
    "\n",
    "In this step, we embed full clinical notes and store them in a **FAISS** vector store, which enables efficient similarity search. We use a lightweight transformer model (`MiniLM`) to convert each note into a semantic vector.\n",
    "\n",
    "### Steps:\n",
    "- **2.1**: Embed clinical notes using the `all-MiniLM-L6-v2` model from Hugging Face.\n",
    "- **2.2**: View an embedded document along with its metadata and vector representation.\n",
    "\n",
    "### Why Use This Approach?\n",
    "\n",
    "Storing entire notes is useful when:\n",
    "- You want to preserve the full clinical context for each patient.\n",
    "- Your downstream use case (e.g., summarization or structured extraction) requires complete narrative input.\n",
    "- The notes are concise enough to fit within the input limits of an LLM.\n",
    "\n",
    "This method simplifies retrieval workflows by allowing you to work with whole documents rather than fragmented chunks.\n",
    "\n",
    "<img src=\"./images/rag_full.png\" alt=\"RAG Full\" width=\"900\">\n",
    "\n",
    "\n"
   ],
   "id": "52ad390e417fe45d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2.1. Embed Clinical Notes Using Local MiniLM Embeddings\n",
    "# -----------------------------------------------------------\n",
    "# This cell encodes each clinical note into a vector using a local\n",
    "# transformer model and stores those embeddings in a FAISS index for\n",
    "# fast similarity search.\n",
    "\n",
    "# Model: `sentence-transformers/all-MiniLM-L6-v2`\n",
    "# - Optimized for semantic similarity tasks\n",
    "# - Lightweight and fast (384-dimensional vectors)\n",
    "# - Runs fully offline\n",
    "\n",
    "# Requirements:\n",
    "#   pip install langchain langchain-huggingface sentence-transformers faiss-cpu\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Prepare inputs for embedding: note text and relevant metadata\n",
    "documents = df[\"note_text\"].tolist()\n",
    "metadata = df[[\"patient_num\", \"encounter_num\", \"start_date\"]].to_dict(orient=\"records\")\n",
    "\n",
    "# Create a FAISS vector store from the documents\n",
    "vectorstore = FAISS.from_texts(documents, embedding_model, metadatas=metadata)\n",
    "\n",
    "print(f\"‚úÖ Successfully embedded {len(documents)} clinical notes using MiniLM.\")\n"
   ],
   "id": "cb4d79a45a47b55e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2.2. View a Specific Embedded Document, Metadata, and Vector\n",
    "# -----------------------------------------------------------\n",
    "# Select an index (e.g., id = 5) to inspect the stored document.\n",
    "# This cell shows the document text, associated metadata, and\n",
    "# the corresponding FAISS embedding vector.\n",
    "\n",
    "id = 1  # You can change this index to view a different record\n",
    "\n",
    "# Get (doc_id, Document) tuple from LangChain's docstore\n",
    "doc_id, doc_example = list(vectorstore.docstore._dict.items())[id]\n",
    "\n",
    "# Retrieve corresponding vector from FAISS\n",
    "vector_example = vectorstore.index.reconstruct(id)\n",
    "\n",
    "display(Markdown(f\"### üßæ Document ID: `{doc_id}`\"))\n",
    "display(Markdown(f\"**Metadata:** `{doc_example.metadata}`\"))\n",
    "\n",
    "display(Markdown(\"**Document Text (First 500 characters):**\"))\n",
    "display(Markdown(f\"```text\\n{doc_example.page_content[:500]}...\\n```\"))\n",
    "\n",
    "display(Markdown(\"**Embedded Vector (First 100):**\"))\n",
    "display(Markdown(f\"```text\\n{vector_example[:100]}\\n```\"))"
   ],
   "id": "2e8bf3477478862b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Retrieving Clinical Notes with Similarity Score (RAG Retrieval)\n",
    "\n",
    "In this section, we perform semantic search over embedded clinical notes using a FAISS vector store and a locally generated query vector. We use similarity scores to evaluate the relevance of each match to the query.\n",
    "\n",
    "### Key Retrieval Steps:\n",
    "\n",
    "1. **Embed a Query (Step 3.1)**\n",
    "   - Converts a natural language question into a numerical vector using the same MiniLM model used to embed the notes.\n",
    "\n",
    "2. **Similarity Search with Scores (Step 3.2)**\n",
    "   - Retrieves the top-k clinical notes ranked by cosine similarity to the query.\n",
    "   - Includes similarity scores for transparency and ranking.\n",
    "\n",
    "3. **Score Threshold Filtering (Step 3.3)**\n",
    "   - Filters out matches with low similarity scores.\n",
    "   - Helps improve the precision and clinical relevance of the results.\n",
    "\n",
    "### Why Use These Techniques?\n",
    "\n",
    "Similarity search helps identify notes most relevant to a user-defined question or condition. Threshold filtering ensures:\n",
    "- Only strong matches are considered for downstream tasks like summarization\n",
    "- Noisy or unrelated content is excluded\n",
    "- Each result can be justified based on a similarity score\n",
    "\n",
    "<img src=\"./images/rag_retrieval.png\" alt=\"RAG Retrieval\" width=\"900\">\n"
   ],
   "id": "aeec2830e4432c29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.1. Embed a Query and Inspect Its Vector Representation\n",
    "# -----------------------------------------------------------\n",
    "# This step encodes a natural language query into a numerical vector\n",
    "# using the same MiniLM model used for the clinical notes.\n",
    "# This vector will be used to search for semantically similar notes.\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Define a sample clinical query\n",
    "query = \"Who has asthma and is taking Fluticasone and Albuterol?\"\n",
    "\n",
    "# Generate the embedding for the query\n",
    "query_vector = embedding_model.embed_query(query)\n",
    "\n",
    "# Display the vector and its shape\n",
    "display(Markdown(\"### Vectorized Query\"))\n",
    "display(Markdown(f\"`Query:` *{query}*\"))\n",
    "display(Markdown(\"**Embedding Vector (truncated):**\"))\n",
    "display(Markdown(f\"```text\\n{query_vector[:100]} ... [{len(query_vector)} dimensions]\\n```\"))\n"
   ],
   "id": "b452485e8b120fb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.2. Similarity Search (Top-K Results, No Filtering)\n",
    "# -----------------------------------------------------------\n",
    "# This cell performs a semantic similarity search using the embedded query,\n",
    "# returning the top-k most similar clinical notes along with similarity scores.\n",
    "\n",
    "# Score interpretation:\n",
    "# - 0.90 ‚Äì 1.00: Highly relevant\n",
    "# - 0.70 ‚Äì 0.90: Strong match\n",
    "# - 0.50 ‚Äì 0.70: Moderate match\n",
    "# - 0.30 ‚Äì 0.50: Low match\n",
    "# - 0.00 ‚Äì 0.30: Minimal or irrelevant\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Define number of top results\n",
    "top_k = 5\n",
    "\n",
    "# Run similarity search\n",
    "results = vectorstore.similarity_search_with_score(query, k=top_k)\n",
    "\n",
    "# Display header\n",
    "display(Markdown(f\"### üîç Top {top_k} Most Similar Clinical Notes\"))\n",
    "\n",
    "# Iterate and display each match\n",
    "for i, (doc, score) in enumerate(results):\n",
    "    display(Markdown(f\"---\\n**Result {i+1}**  \\n- **Similarity Score:** `{score:.4f}`  \\n- **Patient Num:** `{doc.metadata.get('patient_num', 'N/A')}`  \\n- **Encounter:** `{doc.metadata.get('encounter_num', 'N/A')}`\\n\\n**Note Preview:**\\n```text\\n{doc.page_content[:1200]}\\n```\"))\n"
   ],
   "id": "10ff12b605678727",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.3. Filter Search Results by Similarity Score Threshold\n",
    "# -----------------------------------------------------------\n",
    "# This cell filters the top-K search results to keep only those\n",
    "# with high similarity scores above a defined threshold.\n",
    "\n",
    "# Similarity Score Threshold:\n",
    "# - Only notes with scores ‚â• threshold will be retained.\n",
    "# - Higher scores = greater semantic similarity.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "threshold = 0.55  # Keep notes with score ‚â• 0.55\n",
    "\n",
    "# Initialize an empty list to hold the filtered results\n",
    "filtered_results = []\n",
    "\n",
    "# Loop through each result (a tuple of Document and score)\n",
    "for doc, score in results:\n",
    "    # Check if the similarity score meets the threshold\n",
    "    if score >= threshold:\n",
    "        # If so, add it to the filtered list\n",
    "        filtered_results.append((doc, score))\n",
    "\n",
    "\n",
    "# Summary\n",
    "display(Markdown(f\"### ‚úÖ {len(filtered_results)} of {top_k} notes passed the similarity threshold (‚â• {threshold})\"))\n",
    "\n",
    "# Show filtered results\n",
    "for i, (doc, score) in enumerate(filtered_results):\n",
    "    display(Markdown(\n",
    "        f\"---\\n**Filtered Match {i+1}**  \\n\"\n",
    "        f\"- **Similarity Score:** `{score:.4f}`  \\n\"\n",
    "        f\"- **Patient Num:** `{doc.metadata.get('patient_num', 'N/A')}`  \\n\"\n",
    "        f\"- **Encounter:** `{doc.metadata.get('encounter_num', 'N/A')}`\\n\\n\"\n",
    "        f\"**Note Preview:**\\n```text\\n{doc.page_content[:1200]}\\n```\"\n",
    "    ))\n"
   ],
   "id": "e759fdce7afea1b1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Generating Structured Responses with an LLM (RAG Retrieval)\n",
    "\n",
    "In this section, we take the clinical notes retrieved via semantic search and pass them into a Large Language Model (LLM) to generate structured, clinically meaningful responses. This is the final step in the **Retrieval-Augmented Generation (RAG)** pipeline.\n",
    "\n",
    "### Key Steps:\n",
    "\n",
    "1. **Creating a Prompt Template for LLM Querying (Step 4.1)**\n",
    "   - Defines a reusable prompt structure for analyzing and summarizing clinical notes.\n",
    "   - Ensures each response includes patient metadata and clear, structured outputs.\n",
    "\n",
    "2. **Invoking LLM with Retrieved Context (Step 4.2)**\n",
    "   - Inserts the top-retrieved clinical notes into the prompt.\n",
    "   - Sends the prompt to a local LLM (e.g., Qwen2 via Ollama) for structured generation.\n",
    "   - Returns a summary that directly answers the user‚Äôs medical query.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "This step demonstrates how LLMs can synthesize information from real patient notes to produce:\n",
    "- Patient-specific summaries\n",
    "- Answered clinical questions\n",
    "- Traceable outputs with structured identifiers\n",
    "\n",
    "This capability is essential for use cases like clinical decision support, patient-facing summaries, or intelligent search interfaces.\n",
    "\n",
    "<img src=\"./images/rag_generation.png\" alt=\"RAG Generation\" width=\"1250\">\n"
   ],
   "id": "f36c72898d6bab9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4.1. Create a Prompt Template for LLM Querying\n",
    "# -----------------------------------------------------------\n",
    "# This prompt template guides the LLM to generate structured summaries\n",
    "# from clinical notes retrieved via similarity search.\n",
    "\n",
    "# It includes placeholders for:\n",
    "# - {retrieved_docs}: Injects the top-matching clinical notes\n",
    "# - {query}: A user-defined clinical question\n",
    "\n",
    "# Output Expectations:\n",
    "# - One structured response per patient\n",
    "# - Includes metadata for traceability\n",
    "# - Summarizes and answers the query based on each patient's most recent note\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"You are a medical assistant analyzing clinical notes. Based on the following records:\\n\\n\"\n",
    "    \"{retrieved_docs}\\n\\n\"\n",
    "    \"Answer the question: {query} using the following structure:\\n\"\n",
    "    \"   - Patient Num: <value>, Gender: <value>, Age: <value>, Race: <value>\\n\"\n",
    "    \"   - Visit Date: <value>\\n\"\n",
    "    \"   - Summary: One paragraph summarizing the patient note and answering the question.\\n\\n\"\n",
    "    \"   - Has Asthma: <Yes/No>\"\n",
    "    \"Instructions:\\n\"\n",
    "    \"- Show all patients that are relevant to the query.\\n\"\n",
    "    \"- Only consider the most recent note for each patient (identified by patient_num).\"\n",
    ")\n",
    "\n",
    "display(Markdown(f\"```\\n{prompt_template.template}\\n```\"))\n"
   ],
   "id": "ebb9aec086115c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4.2. Use Retrieved Context to Invoke LLM and Generate Response\n",
    "# -----------------------------------------------------------\n",
    "# This cell completes the RAG workflow by injecting the top-matching clinical notes\n",
    "# into a prompt template and invoking a local LLM to generate a structured response.\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Initialize the local LLM (ensure this model has been pulled via Ollama)\n",
    "model = ChatOllama(model=\"qwen2\")\n",
    "\n",
    "# Prepare the context by joining top retrieved notes\n",
    "retrieved_context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in filtered_results])\n",
    "\n",
    "# Fill in the prompt template with the retrieved notes and query\n",
    "final_prompt = prompt_template.format(\n",
    "    retrieved_docs=retrieved_context,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "# Run inference using the LLM\n",
    "response = model.invoke(final_prompt)\n",
    "\n",
    "# Display the generated response\n",
    "display(Markdown(\"### üìã LLM-Generated Response\"))\n",
    "display(Markdown(response.content))\n"
   ],
   "id": "44506dfe18ccb7c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
