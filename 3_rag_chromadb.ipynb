{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "![i2b2 Logo](images/transmart-logo.png)\n",
    "\n",
    "# Using ChromaDB + Embeddings to Search Patient Notes (RAG)\n",
    "\n",
    "This notebook demonstrates how to implement a **Retrieval-Augmented Generation (RAG)** pipeline using **local embeddings** and **ChromaDB** to search and analyze clinical notes stored in an i2b2-like format. It walks through decoding, embedding, storing, retrieving, and generating responses with a local LLM.\n",
    "\n",
    "You will work through a complete pipeline:\n",
    "- Decode notes from BinHex format\n",
    "- Embed clinical notes using the MiniLM model\n",
    "- Store the notes and metadata in a persistent ChromaDB vector store\n",
    "- Perform semantic similarity search and inspect cosine-based scores\n",
    "- Apply **Maximal Marginal Relevance (MMR)** to promote diversity and reduce redundancy\n",
    "- Retrieve only the **most recent note per patient** for clarity and accuracy\n",
    "- Inject results into a reusable prompt template\n",
    "- Generate structured, clinically meaningful responses using a local LLM (e.g., Qwen or LLaMA 3 via Ollama)\n",
    "\n",
    "### 🔍 Key Concepts Covered\n",
    "\n",
    "- Decoding i2b2-formatted BinHex clinical notes\n",
    "- Embedding full notes for semantic similarity search\n",
    "- Persistent storage and retrieval using ChromaDB\n",
    "- Score-based filtering and MMR ranking\n",
    "- Structured prompt engineering for LLMs\n",
    "- Zero-shot summarization with a local model\n",
    "\n",
    "Each cell builds on the last to demonstrate a complete RAG workflow, tailored to **clinical informatics and patient note analysis**.\n",
    "\n",
    "> This notebook is part of the workshop: _Using LLMs to Search and Summarize Patient Notes_.\n"
   ],
   "id": "e6022b84adefe37b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Prepare Data for Embedding\n",
    "\n",
    "Before we can perform semantic search or generate responses using clinical notes, we must first prepare the data.\n",
    "\n",
    "- **1.1**: Load a simulated i2b2 `visit_dimension` table that contains BinHex-encoded clinical notes and decode them into readable plain text.\n",
    "\n",
    "This step ensures that each clinical note is in a usable format for embedding in the next section using a local MiniLM model.\n"
   ],
   "id": "9a4858c32f021553"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 1.1. Load and Decode Clinical Notes from i2b2-Mimicking CSV\n",
    "# -----------------------------------------------------------\n",
    "# This cell loads visit-level data from a CSV file that mimics the i2b2 `visit_dimension` table\n",
    "# and decodes the BinHex-encoded clinical notes into readable plain text.\n",
    "\n",
    "# Each record includes:\n",
    "# - encounter_num: Unique visit ID\n",
    "# - patient_num: Patient identifier\n",
    "# - start_date, end_date: Visit dates\n",
    "# - location_cd, location_path: Clinic metadata\n",
    "# - visit_blob: BinHex-encoded clinical note text\n",
    "# - note_text: Decoded BinHex clinical note text\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import binascii\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Load the i2b2-style data\n",
    "csv_path = \"datafiles/i2b2_encounter_table.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Function to decode BinHex-encoded notes\n",
    "def decode_note(hex_blob):\n",
    "    hex_str = hex_blob.replace(\"0x\", \"\")\n",
    "    return binascii.unhexlify(hex_str).decode(\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "# Decode all notes into a new column\n",
    "df[\"note_text\"] = df[\"visit_blob\"].apply(decode_note)\n",
    "\n",
    "# Preview the first 10 records with decoded notes\n",
    "display(Markdown(\"### Preview of Decoded Notes\"))\n",
    "display(df.head(10))\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Embed and Store Entire Clinical Notes in ChromaDB\n",
    "\n",
    "In this step, we process and store each clinical note as a **complete document** in a ChromaDB vector store. This approach preserves full patient context, making it ideal for semantic search and downstream clinical reasoning.\n",
    "\n",
    "### Step 2.1: Embed and Store Notes\n",
    "1. **Embed Full Notes**\n",
    "   Each note is converted into a semantic vector using a lightweight transformer model (`MiniLM`).\n",
    "\n",
    "2. **Store in ChromaDB**\n",
    "   The embedded vector and its associated metadata (e.g., patient ID, encounter number, visit date) are stored together in a persistent ChromaDB directory.\n",
    "\n",
    "### Why This Approach?\n",
    "\n",
    "Storing full notes is especially useful when:\n",
    "- You want to retrieve the complete narrative for clinical context\n",
    "- Your downstream task (e.g., summarization or decision support) depends on comprehensive input\n",
    "- Each note fits within the input limit of a single LLM call\n",
    "\n",
    "This setup supports more faithful summarization and reasoning than chunk-based approaches when notes are relatively short and self-contained.\n",
    "\n",
    "<img src=\"./images/rag_full.png\" alt=\"RAG Full\" width=\"900\">\n"
   ],
   "id": "437c683321f7757e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 2.1. Embed Clinical Notes Using Local MiniLM Embeddings and Store in ChromaDB\n",
    "# -----------------------------------------------------------\n",
    "# This cell encodes each clinical note into a semantic vector using a local MiniLM model\n",
    "# and stores the results along with metadata in a ChromaDB vector store for retrieval.\n",
    "\n",
    "# Embedding Model: sentence-transformers/all-MiniLM-L6-v2\n",
    "# - Optimized for fast local execution\n",
    "# - Produces 384-dimensional vectors for semantic similarity\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# Initialize the embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "# Create or connect to the persistent ChromaDB directory\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./datafiles/chroma_db_notes\",\n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Prepare documents and metadata\n",
    "documents = df[\"note_text\"].tolist()\n",
    "metadata = df[[\"patient_num\", \"encounter_num\", \"start_date\"]].to_dict(orient=\"records\")\n",
    "\n",
    "# Add text and metadata to the ChromaDB vector store\n",
    "vectorstore.add_texts(texts=documents, metadatas=metadata)\n",
    "\n",
    "print(f\"✅ Successfully embedded and stored {len(documents)} clinical notes using MiniLM and ChromaDB.\")\n"
   ],
   "id": "cb4d79a45a47b55e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Retrieving Clinical Notes with Similarity Score (RAG Retrieval with ChromaDB)\n",
    "\n",
    "In this section, we retrieve relevant clinical notes from a ChromaDB vector store using vector similarity techniques. We explore both standard and advanced retrieval strategies to improve the relevance and diversity of retrieved context for downstream LLM generation.\n",
    "\n",
    "### Key Retrieval Strategies\n",
    "\n",
    "1. **Define a Clinical Query (Step 3.1)**\n",
    "   - The user provides a natural language question (e.g., \"Who has asthma and is taking Fluticasone and Albuterol?\").\n",
    "\n",
    "2. **Similarity Search with Scores (Step 3.2)**\n",
    "   - Retrieves the top-k notes ranked by semantic similarity to the query.\n",
    "   - Returns cosine-based relevance scores for each match.\n",
    "\n",
    "3. **Score Threshold Filtering (Step 3.3)**\n",
    "   - Filters out low-confidence matches using a minimum similarity score.\n",
    "   - Retains only notes that meet a defined relevance threshold (e.g., ≥ 0.6).\n",
    "\n",
    "4. **Maximal Marginal Relevance (MMR) Search (Step 3.4)**\n",
    "   - Balances similarity and diversity.\n",
    "   - Reduces redundancy by selecting a diverse subset of highly relevant notes.\n",
    "\n",
    "### Why Use These Strategies?\n",
    "\n",
    "High-quality retrieval is critical to the success of RAG workflows. These techniques help:\n",
    "- Improve the contextual relevance of inputs to the LLM\n",
    "- Filter out irrelevant or low-confidence documents\n",
    "- Encourage diverse results to reduce bias and improve robustness\n",
    "\n",
    "<img src=\"./images/rag_retrieval.png\" alt=\"RAG Retrieval\" width=\"900\">\n"
   ],
   "id": "b43a3635d3cf133c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.1. Define the Query for Clinical Note Retrieval\n",
    "# -----------------------------------------------------------\n",
    "# This cell defines a natural language query to search the embedded clinical notes\n",
    "# stored in ChromaDB using semantic similarity.\n",
    "\n",
    "# Concept:\n",
    "# The query will be *** automatically embedded *** by the vectorstore before performing\n",
    "# a semantic comparison against stored note vectors.\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Example Clinical Query:\n",
    "query = \"Who has asthma and is taking Fluticasone or Albuterol?\"\n",
    "\n",
    "# Display the query for reference\n",
    "display(Markdown(f\"### 🔍 Query: `{query}`\"))\n"
   ],
   "id": "387fe73e7a1ed4d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.2. Perform Similarity Search with Relevance Scores\n",
    "# -----------------------------------------------------------\n",
    "# This cell retrieves the top-k clinical notes most semantically similar to the input query.\n",
    "# Each result includes a cosine similarity score returned by LangChain's Chroma vectorstore wrapper.\n",
    "\n",
    "# Function used:\n",
    "# - vectorstore.similarity_search_with_relevance_scores(query, k=10)\n",
    "#   Returns a list of (Document, score) tuples.\n",
    "\n",
    "# Relevance Score Interpretation (heuristic only):\n",
    "# - 0.90 – 1.00 → Highly relevant\n",
    "# - 0.70 – 0.90 → Strong relevance\n",
    "# - 0.50 – 0.70 → Moderate relevance\n",
    "# - 0.30 – 0.50 → Low relevance\n",
    "# - 0.00 – 0.30 → Minimal or no relevance\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Perform the similarity search\n",
    "results = vectorstore.similarity_search_with_relevance_scores(query, k=10)\n",
    "\n",
    "# Display the results\n",
    "display(Markdown(\"### 🔍 Retrieved Clinical Notes with Relevance Scores\"))\n",
    "\n",
    "for idx, (doc, score) in enumerate(results, 1):\n",
    "    patient = doc.metadata.get(\"patient_num\", \"N/A\")\n",
    "    encounter = doc.metadata.get(\"encounter_num\", \"N/A\")\n",
    "    date = doc.metadata.get(\"start_date\", \"N/A\")\n",
    "    doc_id = doc.id\n",
    "    excerpt = doc.page_content[:1000].replace(\"\\n\", \" \")\n",
    "\n",
    "    display(Markdown(\n",
    "        f\"---\\n**Result {idx}**  \\n\"\n",
    "        f\"- **Relevance Score:** `{score:.4f}`  \\n\"\n",
    "        f\"- **Patient Num:** `{patient}`  \\n\"\n",
    "        f\"- **Encounter Num:** `{encounter}`  \\n\"\n",
    "        f\"- **Start Date:** `{date}`  \\n\"\n",
    "        f\"- **Document ID:** `{doc_id}`  \\n\\n\"\n",
    "        f\"**Excerpt:**\\n```text\\n{excerpt}...\\n```\"\n",
    "    ))\n"
   ],
   "id": "722f9bd17a7fe54e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.3. Use a Retriever with a Similarity Score Threshold\n",
    "# -----------------------------------------------------------\n",
    "# This cell configures a retriever that only returns documents whose\n",
    "# cosine similarity scores exceed a predefined threshold.\n",
    "\n",
    "# Parameters:\n",
    "# - search_type=\"similarity_score_threshold\": Enables threshold-based filtering.\n",
    "# - search_kwargs={\"k\": 10, \"score_threshold\": 0.6}\n",
    "#     - k: Maximum number of documents to *evaluate* (not necessarily return)\n",
    "#     - score_threshold: Minimum similarity score (0–1) required for inclusion\n",
    "\n",
    "# Purpose:\n",
    "# Improves retrieval precision by excluding weak semantic matches—\n",
    "# especially important for clinical reasoning and safety-critical applications.\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Define threshold and top-k limit\n",
    "score_threshold = 0.59\n",
    "top_k = 10\n",
    "\n",
    "# Create the retriever with filtering\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={\"k\": top_k, \"score_threshold\": score_threshold}\n",
    ")\n",
    "\n",
    "# Execute the retrieval\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Display results\n",
    "display(Markdown(f\"### 🔎 Retrieved Clinical Notes (Score ≥ {score_threshold})\"))\n",
    "\n",
    "for idx, doc in enumerate(results, 1):\n",
    "    patient = doc.metadata.get(\"patient_num\", \"N/A\")\n",
    "    date = doc.metadata.get(\"start_date\", \"N/A\")\n",
    "    doc_id = doc.id\n",
    "    excerpt = doc.page_content[:1000].replace(\"\\n\", \" \")\n",
    "\n",
    "    display(Markdown(\n",
    "        f\"**Document {idx}**  \\n\"\n",
    "        f\"- **Patient Num:** `{patient}`  \\n\"\n",
    "        f\"- **Start Date:** `{date}`  \\n\"\n",
    "        f\"- **Document ID:** `{doc_id}`  \\n\\n\"\n",
    "        f\"**Excerpt:**\\n```text\\n{excerpt}...\\n```\"\n",
    "    ))\n",
    "\n",
    "display(Markdown(f\"**✅ Total relevant results:** `{len(results)}`\"))\n"
   ],
   "id": "f1aca9739171e3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 3.4. Perform Maximal Marginal Relevance (MMR) Search\n",
    "# -----------------------------------------------------------\n",
    "# This cell retrieves clinical notes using Maximal Marginal Relevance (MMR),\n",
    "# an approach that balances query relevance with result diversity.\n",
    "\n",
    "# Parameters:\n",
    "# - fetch_k = 500: Number of top-ranked candidates to evaluate before reranking\n",
    "# - k = 5: Final number of diverse, relevant documents to return\n",
    "# - lambda_mult:\n",
    "#     - 0.0 → prioritize diversity (minimal redundancy)\n",
    "#     - 1.0 → prioritize similarity to the query\n",
    "#     - 0.5 → balanced trade-off between diversity and relevance\n",
    "\n",
    "# Purpose:\n",
    "# MMR reduces redundancy by penalizing near-duplicate documents, while still\n",
    "# prioritizing clinical notes that are highly relevant to the query.\n",
    "# This is especially useful in settings where multiple perspectives or\n",
    "# treatment variations are valuable (e.g., different asthma management plans).\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Perform MMR search\n",
    "results = vectorstore.max_marginal_relevance_search(\n",
    "    query=query,\n",
    "    k=5,\n",
    "    fetch_k=500,\n",
    "    lambda_mult=0.5\n",
    ")\n",
    "\n",
    "# Display results\n",
    "display(Markdown(\"### 📘 Retrieved Clinical Notes Using Maximal Marginal Relevance (MMR)\"))\n",
    "\n",
    "for idx, doc in enumerate(results, 1):\n",
    "    patient = doc.metadata.get(\"patient_num\", \"N/A\")\n",
    "    date = doc.metadata.get(\"start_date\", \"N/A\")\n",
    "    doc_id = getattr(doc, \"id\", \"N/A\")\n",
    "    excerpt = doc.page_content[:1000].replace(\"\\n\", \" \")\n",
    "\n",
    "    display(Markdown(\n",
    "        f\"**Document {idx}**  \\n\"\n",
    "        f\"- **Patient Num:** `{patient}`  \\n\"\n",
    "        f\"- **Start Date:** `{date}`  \\n\"\n",
    "        f\"- **Document ID:** `{doc_id}`  \\n\\n\"\n",
    "        f\"**Excerpt:**\\n```text\\n{excerpt}...\\n```\"\n",
    "    ))\n",
    "\n",
    "display(Markdown(f\"**✅ Total MMR Results Returned:** `{len(results)}`\"))\n"
   ],
   "id": "828479f4c3baa9bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Generating Structured Responses with an LLM (RAG Retrieval)\n",
    "\n",
    "In this final step, we use a Large Language Model (LLM) to analyze the clinical notes retrieved in the previous section. By injecting these relevant notes into a structured prompt, we enable the LLM to generate clinically useful, structured responses.\n",
    "\n",
    "This completes the Retrieval-Augmented Generation (RAG) pipeline, connecting search results to intelligent language generation.\n",
    "\n",
    "### Key Steps\n",
    "\n",
    "1. **Create a Prompt Template (Step 4.1)**\n",
    "   - Defines a reusable prompt structure that includes placeholders for both the query and the retrieved clinical context.\n",
    "   - Specifies a structured output format, including metadata fields such as patient ID, encounter date, and a clinical summary.\n",
    "\n",
    "2. **Invoke the LLM with Retrieved Context (Step 4.2)**\n",
    "   - Fills the prompt with retrieved notes and the clinical query.\n",
    "   - Sends the prompt to a local LLM (e.g., Qwen2 or LLaMA 3 via Ollama).\n",
    "   - Returns a structured, context-aware response to the clinical question.\n",
    "\n",
    "### Why It Matters\n",
    "\n",
    "This generation step demonstrates the power of combining semantic search with generative AI:\n",
    "- Produces context-rich answers grounded in patient data\n",
    "- Supports clinical summarization and decision support\n",
    "- Enables zero-shot reasoning over real-world clinical notes\n",
    "\n",
    "<img src=\"./images/rag_generation.png\" alt=\"RAG Generation\" width=\"1250\">\n"
   ],
   "id": "60a469cbbda2e7de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4.1. Create a Prompt Template for LLM Querying\n",
    "# -----------------------------------------------------------\n",
    "# This chat prompt guides the LLM to generate structured clinical summaries\n",
    "# from notes retrieved via similarity search.\n",
    "\n",
    "# Placeholders:\n",
    "# - {retrieved_docs}: Injects top-matching clinical notes\n",
    "# - {query}: A user-defined clinical question\n",
    "\n",
    "# Output Expectations:\n",
    "# - One structured summary per patient\n",
    "# - Metadata included for traceability\n",
    "# - Response based only on the most recent note per patient\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\n",
    "     \"You are an advanced medical documentation assistant. Your role is to extract structured clinical details and summarize findings from retrieved patient notes.\"),\n",
    "\n",
    "    (\"human\",\n",
    "    \"\"\"Based on the following retrieved clinical records:\n",
    "\n",
    "{retrieved_docs}\n",
    "\n",
    "Answer the clinical question: {query}\n",
    "\n",
    "For each relevant patient, provide:\n",
    "\n",
    "1. **Patient Demographics**: Patient Num, Gender, Age, Race\n",
    "2. **Visit Date**: Use the most recent date only\n",
    "3. **Chief Complaints**\n",
    "4. **Current Medications**\n",
    "5. **Has Asthma?** (Yes/No)\n",
    "   - Only answer 'Yes' if asthma is explicitly stated in the diagnosis OR if the patient is currently prescribed a known asthma medication (e.g., albuterol, fluticasone).\n",
    "   - If asthma is not clearly mentioned or supported by medication, answer 'No'. Do not infer.\n",
    "\n",
    "6. **Summary**: One paragraph summarizing the patient's condition and addressing the query.\n",
    "\n",
    "**Instructions:**\n",
    "- Format your output using clear bullet points.\n",
    "- Include only one entry per patient, using their most recent clinical note.\n",
    "- Avoid including duplicate patients.\n",
    "\"\"\")\n",
    "])\n",
    "\n",
    "print(\"✅ Prompt created and ready to use.\")\n"
   ],
   "id": "ebb9aec086115c34",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# -----------------------------------------------------------\n",
    "# 4.2. Use Retrieved Context to Invoke LLM and Generate Response\n",
    "# -----------------------------------------------------------\n",
    "# This step completes the RAG workflow by injecting the retrieved clinical notes\n",
    "# into a structured chat prompt and using an LLM (via Ollama) to generate a response.\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize the local Ollama model (e.g., Qwen2, LLaMA 3, etc.)\n",
    "model = ChatOllama(model=\"qwen2\")\n",
    "\n",
    "# Prepare the context text (combine page_content from results list)\n",
    "retrieved_context = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in results])\n",
    "\n",
    "# Format chat messages using the ChatPromptTemplate\n",
    "messages = prompt_template.format_messages(\n",
    "    retrieved_docs=retrieved_context,\n",
    "    query=query\n",
    ")\n",
    "\n",
    "# Generate a structured response using the LLM\n",
    "response = model.invoke(messages)\n",
    "\n",
    "# Display the LLM-generated output\n",
    "display(Markdown(\"### 🧠 LLM-Generated Response\"))\n",
    "display(Markdown(response.content))\n"
   ],
   "id": "44506dfe18ccb7c6",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
